{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharika-Saha/ML-hackathon-Group-1/blob/main/HMMCount_w_rl_not_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cYiw72pKa7WH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "602cafac-d785-4d9b-e6af-ed73467f40ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n",
            "Downloading hmmlearn-0.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/166.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m163.8/166.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install hmmlearn\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "# Step 2: Import libraries\n",
        "import numpy as np\n",
        "import string\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from hmmlearn import hmm\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this cell after your imports section in HMMCount.ipynb\n",
        "\n",
        "# =============================================================================\n",
        "# RL COMPONENTS INTEGRATION FROM rl.ipynb\n",
        "# =============================================================================\n",
        "\n",
        "# Additional imports for RL components\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import random\n",
        "from collections import deque\n",
        "import math\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# RL Utilities and Environment\n",
        "class HangmanRLEnvironment:\n",
        "    def __init__(self, words, max_lives=6, hmm_model=None):\n",
        "        self.words = [w.strip().upper() for w in words if w.strip()]\n",
        "        self.max_lives = max_lives\n",
        "        self.hmm_model = hmm_model\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, word=None):\n",
        "        self.target_word = word or random.choice(self.words)\n",
        "        self.masked_word = ['_'] * len(self.target_word)\n",
        "        self.guessed_letters = set()\n",
        "        self.wrong_letters = set()\n",
        "        self.lives = self.max_lives\n",
        "        self.done = False\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        # Convert game state to numerical representation for RL\n",
        "        state = []\n",
        "\n",
        "        # Positional information (26 letters * 20 positions max)\n",
        "        max_len = 20\n",
        "        for pos in range(max_len):\n",
        "            for letter in string.ascii_uppercase:\n",
        "                if pos < len(self.masked_word) and self.masked_word[pos] == letter:\n",
        "                    state.append(1.0)  # Letter revealed\n",
        "                elif letter in self.guessed_letters:\n",
        "                    state.append(-1.0) # Letter guessed but not here\n",
        "                else:\n",
        "                    state.append(0.0)  # Letter not guessed\n",
        "\n",
        "        # Global letter information\n",
        "        for letter in string.ascii_uppercase:\n",
        "            if letter in self.guessed_letters:\n",
        "                state.append(-1.0)\n",
        "            else:\n",
        "                state.append(0.0)\n",
        "\n",
        "        # Game progress information\n",
        "        state.append(len(self.guessed_letters) / 26.0)\n",
        "        state.append(self.lives / self.max_lives)\n",
        "        state.append(self.masked_word.count('_') / max(1, len(self.masked_word)))\n",
        "\n",
        "        return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        letter = string.ascii_uppercase[action]\n",
        "        reward = -0.05  # Small step penalty\n",
        "        info = {}\n",
        "\n",
        "        if letter in self.guessed_letters:\n",
        "            return self._get_state(), reward - 2.0, self.done, {\"repeated\": True}\n",
        "\n",
        "        self.guessed_letters.add(letter)\n",
        "\n",
        "        if letter in self.target_word:\n",
        "            # Update masked word\n",
        "            for i, char in enumerate(self.target_word):\n",
        "                if char == letter:\n",
        "                    self.masked_word[i] = letter\n",
        "\n",
        "            # Positive reward for correct guess\n",
        "            letters_revealed = sum(1 for char in self.target_word if char == letter)\n",
        "            reward += letters_revealed * 2\n",
        "\n",
        "            # Check if game is won\n",
        "            if '_' not in self.masked_word:\n",
        "                reward += 5.0\n",
        "                self.done = True\n",
        "                info[\"win\"] = True\n",
        "        else:\n",
        "            self.wrong_letters.add(letter)\n",
        "            self.lives -= 1\n",
        "            reward -= 1.0\n",
        "            info[\"wrong\"] = True\n",
        "\n",
        "            if self.lives <= 0:\n",
        "                reward -= 3.0\n",
        "                self.done = True\n",
        "                info[\"lose\"] = True\n",
        "\n",
        "        return self._get_state(), reward, self.done, info\n",
        "\n",
        "# DQN Network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[256, 128], output_dim=26):\n",
        "        super(DQN, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim=26, lr=1e-3, gamma=0.99, epsilon=1.0,\n",
        "                 epsilon_min=0.01, epsilon_decay=0.995, memory_size=10000):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        # Neural networks\n",
        "        self.policy_net = DQN(state_dim)\n",
        "        self.target_net = DQN(state_dim)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def act(self, state, available_actions):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.choice(available_actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "                q_values = self.policy_net(state_tensor).squeeze().numpy()\n",
        "\n",
        "                # Mask unavailable actions\n",
        "                masked_q = q_values.copy()\n",
        "                for action in range(self.action_dim):\n",
        "                    if action not in available_actions:\n",
        "                        masked_q[action] = -np.inf\n",
        "\n",
        "                return np.argmax(masked_q)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.BoolTensor(dones)\n",
        "\n",
        "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
        "        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
        "\n",
        "        loss = self.loss_fn(current_q_values.squeeze(), target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy_net.state_dict(),\n",
        "            'target_state_dict': self.target_net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon\n",
        "        }, filepath)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "\n",
        "# Q-Learning Agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.q_table = defaultdict(lambda: np.zeros(26))\n",
        "\n",
        "    def get_state_key(self, masked_word, guessed_letters, lives):\n",
        "        # Create a simplified state representation for Q-learning\n",
        "        revealed = ''.join([c if c != '_' else '?' for c in masked_word])\n",
        "        guessed_str = ''.join(sorted(guessed_letters))\n",
        "        return f\"{revealed}_{guessed_str}_{lives}\"\n",
        "\n",
        "    def act(self, state_key, available_actions):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(available_actions)\n",
        "        else:\n",
        "            q_values = self.q_table[state_key]\n",
        "            masked_q = q_values.copy()\n",
        "            for action in range(26):\n",
        "                if action not in available_actions:\n",
        "                    masked_q[action] = -np.inf\n",
        "            return np.argmax(masked_q)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        current_q = self.q_table[state][action]\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
        "\n",
        "        self.q_table[state][action] += self.alpha * (target - current_q)\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save(self, filepath):\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(dict(self.q_table), f)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        with open(filepath, 'rb') as f:\n",
        "            self.q_table = defaultdict(lambda: np.zeros(26), pickle.load(f))\n",
        "\n",
        "# RL-Enhanced Hybrid Model\n",
        "class RLEnhancedHybridModel:\n",
        "    def __init__(self, hybrid_hmm_model, dqn_path='dqn.pt', q_table_path='q_table.pkl',\n",
        "                 use_dqn=True, use_qtable=True, rl_weight=0.3):\n",
        "        self.hybrid_model = hybrid_hmm_model\n",
        "        self.use_dqn = use_dqn\n",
        "        self.use_qtable = use_qtable\n",
        "        self.rl_weight = rl_weight\n",
        "\n",
        "        # Load trained RL models\n",
        "        self.dqn_agent = None\n",
        "        self.q_agent = None\n",
        "\n",
        "        if use_dqn:\n",
        "            try:\n",
        "                self.dqn_agent = DQNAgent(state_dim=546)  # Adjust based on your state dimension\n",
        "                self.dqn_agent.load(dqn_path)\n",
        "                print(\"‚úÖ DQN agent loaded successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Failed to load DQN: {e}\")\n",
        "                self.use_dqn = False\n",
        "\n",
        "        if use_qtable:\n",
        "            try:\n",
        "                self.q_agent = QLearningAgent()\n",
        "                self.q_agent.load(q_table_path)\n",
        "                print(\"‚úÖ Q-table agent loaded successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Failed to load Q-table: {e}\")\n",
        "                self.use_qtable = False\n",
        "\n",
        "        print(f\"üéØ RL-Enhanced Hybrid Model initialized with RL weight: {rl_weight}\")\n",
        "\n",
        "    def get_letter_probabilities(self, masked_word, guessed_letters):\n",
        "        # Get HMM probabilities\n",
        "        hmm_probs = self.hybrid_model.get_letter_probabilities(masked_word, guessed_letters)\n",
        "\n",
        "        if not (self.use_dqn or self.use_qtable):\n",
        "            return hmm_probs\n",
        "\n",
        "        # Get RL probabilities\n",
        "        rl_probs = self._get_rl_probabilities(masked_word, guessed_letters)\n",
        "\n",
        "        if not rl_probs:\n",
        "            return hmm_probs\n",
        "\n",
        "        # Combine HMM and RL probabilities\n",
        "        combined_probs = {}\n",
        "        available_letters = [l for l in string.ascii_uppercase if l not in guessed_letters]\n",
        "\n",
        "        for letter in available_letters:\n",
        "            hmm_prob = hmm_probs.get(letter, 0)\n",
        "            rl_prob = rl_probs.get(letter, 0)\n",
        "\n",
        "            # Weighted combination\n",
        "            combined_prob = (1 - self.rl_weight) * hmm_prob + self.rl_weight * rl_prob\n",
        "            combined_probs[letter] = combined_prob\n",
        "\n",
        "        # Normalize\n",
        "        total = sum(combined_probs.values())\n",
        "        if total > 0:\n",
        "            combined_probs = {k: v/total for k, v in combined_probs.items()}\n",
        "\n",
        "        return combined_probs\n",
        "\n",
        "    def _get_rl_probabilities(self, masked_word, guessed_letters):\n",
        "        available_actions = [string.ascii_uppercase.index(l) for l in string.ascii_uppercase\n",
        "                           if l not in guessed_letters]\n",
        "\n",
        "        if not available_actions:\n",
        "            return {}\n",
        "\n",
        "        rl_scores = {}\n",
        "\n",
        "        # Get DQN scores\n",
        "        if self.use_dqn and self.dqn_agent:\n",
        "            # Create state representation (simplified)\n",
        "            state = np.zeros(546)  # Adjust based on your state dimension\n",
        "            # You would need to implement proper state encoding here\n",
        "\n",
        "            try:\n",
        "                dqn_action = self.dqn_agent.act(state, available_actions)\n",
        "                for action in available_actions:\n",
        "                    # Simplified: higher score for DQN's preferred action\n",
        "                    score = 1.0 if action == dqn_action else 0.1\n",
        "                    letter = string.ascii_uppercase[action]\n",
        "                    rl_scores[letter] = rl_scores.get(letter, 0) + score\n",
        "            except Exception as e:\n",
        "                print(f\"DQN inference error: {e}\")\n",
        "\n",
        "        # Get Q-table scores\n",
        "        if self.use_qtable and self.q_agent:\n",
        "            state_key = self.q_agent.get_state_key(''.join(masked_word), guessed_letters, 6)\n",
        "            try:\n",
        "                q_action = self.q_agent.act(state_key, available_actions)\n",
        "                for action in available_actions:\n",
        "                    q_value = self.q_agent.q_table[state_key][action]\n",
        "                    letter = string.ascii_uppercase[action]\n",
        "                    rl_scores[letter] = rl_scores.get(letter, 0) + max(0, q_value)\n",
        "            except Exception as e:\n",
        "                print(f\"Q-table inference error: {e}\")\n",
        "\n",
        "        # Convert to probabilities\n",
        "        if rl_scores:\n",
        "            total_score = sum(rl_scores.values())\n",
        "            return {letter: score/total_score for letter, score in rl_scores.items()}\n",
        "\n",
        "        return {}\n",
        "\n",
        "# Function to load RL models\n",
        "def load_rl_models(dqn_path='dqn.pt', q_table_path='q_table.pkl'):\n",
        "    \"\"\"Load pre-trained RL models\"\"\"\n",
        "    dqn_agent = None\n",
        "    q_agent = None\n",
        "\n",
        "    # Load DQN\n",
        "    try:\n",
        "        if os.path.exists(dqn_path):\n",
        "            dqn_agent = DQNAgent(state_dim=546)\n",
        "            dqn_agent.load(dqn_path)\n",
        "            print(\"‚úÖ DQN model loaded successfully\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  DQN model file not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading DQN: {e}\")\n",
        "\n",
        "    # Load Q-table\n",
        "    try:\n",
        "        if os.path.exists(q_table_path):\n",
        "            q_agent = QLearningAgent()\n",
        "            q_agent.load(q_table_path)\n",
        "            print(\"‚úÖ Q-table loaded successfully\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Q-table file not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading Q-table: {e}\")\n",
        "\n",
        "    return dqn_agent, q_agent\n",
        "\n",
        "# Enhanced evaluation with RL\n",
        "def rl_enhanced_evaluation(hybrid_model, test_words, num_games=2000, use_rl=True):\n",
        "    \"\"\"Enhanced evaluation using RL-enhanced model\"\"\"\n",
        "\n",
        "    if use_rl:\n",
        "        print(\"üöÄ Initializing RL-Enhanced Model...\")\n",
        "        model = RLEnhancedHybridModel(hybrid_model)\n",
        "    else:\n",
        "        print(\"üîß Using Standard Hybrid Model (No RL)\")\n",
        "        model = hybrid_model\n",
        "\n",
        "    return official_evaluation(model, test_words, num_games)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ü§ñ RL COMPONENTS SUCCESSFULLY INTEGRATED\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Available components:\")\n",
        "print(\"‚úÖ HangmanRLEnvironment - RL training environment\")\n",
        "print(\"‚úÖ DQNAgent - Deep Q-Network agent\")\n",
        "print(\"‚úÖ QLearningAgent - Tabular Q-learning agent\")\n",
        "print(\"‚úÖ RLEnhancedHybridModel - HMM + RL combined model\")\n",
        "print(\"‚úÖ load_rl_models() - Function to load pre-trained RL models\")\n",
        "print(\"‚úÖ rl_enhanced_evaluation() - Enhanced evaluation with RL\")\n",
        "print(\"\\\\nUsage: rl_enhanced_evaluation(hybrid_model, test_words)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a1DfIdpmSn_",
        "outputId": "76ec8321-3ca9-49a2-f2d8-bc2a5f047946"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ü§ñ RL COMPONENTS SUCCESSFULLY INTEGRATED\n",
            "======================================================================\n",
            "Available components:\n",
            "‚úÖ HangmanRLEnvironment - RL training environment\n",
            "‚úÖ DQNAgent - Deep Q-Network agent\n",
            "‚úÖ QLearningAgent - Tabular Q-learning agent\n",
            "‚úÖ RLEnhancedHybridModel - HMM + RL combined model\n",
            "‚úÖ load_rl_models() - Function to load pre-trained RL models\n",
            "‚úÖ rl_enhanced_evaluation() - Enhanced evaluation with RL\n",
            "\\nUsage: rl_enhanced_evaluation(hybrid_model, test_words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load and preprocess the corpus\n",
        "def load_and_preprocess_corpus(file_path):\n",
        "    \"\"\"\n",
        "    Load the corpus and preprocess words\n",
        "    - Convert to uppercase\n",
        "    - Remove words with non-alphabet characters\n",
        "    - Group by word length\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        words = [line.strip().upper() for line in f if line.strip()]\n",
        "\n",
        "    # Filter only alphabetic words\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "\n",
        "    # Group words by length\n",
        "    words_by_length = defaultdict(list)\n",
        "    for word in words:\n",
        "        words_by_length[len(word)].append(word)\n",
        "\n",
        "    print(f\"Total words after preprocessing: {len(words)}\")\n",
        "    print(f\"Word length distribution:\")\n",
        "    for length in sorted(words_by_length.keys()):\n",
        "        print(f\"  Length {length}: {len(words_by_length[length])} words\")\n",
        "\n",
        "    return words, words_by_length\n",
        "\n",
        "# Load the corpus (upload your corpus.txt first)\n",
        "words, words_by_length = load_and_preprocess_corpus('corpus.txt')"
      ],
      "metadata": {
        "id": "j9ecThAGa8jN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cf673f-f05d-45e4-84ba-4ba0fc75da96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words after preprocessing: 49979\n",
            "Word length distribution:\n",
            "  Length 1: 46 words\n",
            "  Length 2: 84 words\n",
            "  Length 3: 388 words\n",
            "  Length 4: 1169 words\n",
            "  Length 5: 2340 words\n",
            "  Length 6: 3755 words\n",
            "  Length 7: 5111 words\n",
            "  Length 8: 6348 words\n",
            "  Length 9: 6787 words\n",
            "  Length 10: 6465 words\n",
            "  Length 11: 5452 words\n",
            "  Length 12: 4292 words\n",
            "  Length 13: 3094 words\n",
            "  Length 14: 2019 words\n",
            "  Length 15: 1226 words\n",
            "  Length 16: 698 words\n",
            "  Length 17: 375 words\n",
            "  Length 18: 174 words\n",
            "  Length 19: 88 words\n",
            "  Length 20: 40 words\n",
            "  Length 21: 16 words\n",
            "  Length 22: 8 words\n",
            "  Length 23: 3 words\n",
            "  Length 24: 1 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Analyze letter frequencies and patterns\n",
        "def analyze_corpus(words_by_length):\n",
        "    \"\"\"\n",
        "    Analyze letter frequencies and positional distributions\n",
        "    \"\"\"\n",
        "    # Overall letter frequency\n",
        "    all_letters = ''.join([''.join(words) for words in words_by_length.values()])\n",
        "    letter_freq = Counter(all_letters)\n",
        "\n",
        "    print(\"Overall letter frequency (top 10):\")\n",
        "    for letter, freq in letter_freq.most_common(10):\n",
        "        print(f\"  {letter}: {freq} ({freq/len(all_letters)*100:.2f}%)\")\n",
        "\n",
        "    # Positional frequency analysis for different lengths\n",
        "    positional_freq = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for length, word_list in words_by_length.items():\n",
        "        for word in word_list:\n",
        "            for pos, letter in enumerate(word):\n",
        "                positional_freq[length][(pos, letter)] += 1\n",
        "\n",
        "    return letter_freq, positional_freq\n",
        "\n",
        "letter_freq, positional_freq = analyze_corpus(words_by_length)"
      ],
      "metadata": {
        "id": "yyviOITja_w0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8ee498-a283-48ca-8fae-deb2428964d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall letter frequency (top 10):\n",
            "  E: 49203 (10.37%)\n",
            "  A: 42089 (8.87%)\n",
            "  I: 42047 (8.86%)\n",
            "  O: 35808 (7.54%)\n",
            "  R: 33577 (7.07%)\n",
            "  N: 33314 (7.02%)\n",
            "  T: 32191 (6.78%)\n",
            "  S: 29044 (6.12%)\n",
            "  L: 27406 (5.77%)\n",
            "  C: 21718 (4.58%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Prepare HMM training data for each word length\n",
        "def prepare_hmm_training_data(words_by_length, min_words_threshold=5):\n",
        "    \"\"\"\n",
        "    Prepare training data for HMMs for each word length\n",
        "    Convert words to numerical sequences for HMM training\n",
        "\n",
        "    Parameters:\n",
        "    - words_by_length: Dictionary of words grouped by length\n",
        "    - min_words_threshold: Minimum number of words required to train HMM for a given length\n",
        "    \"\"\"\n",
        "    # Create letter to index mapping (A=0, B=1, ..., Z=25)\n",
        "    letters = string.ascii_uppercase\n",
        "    letter_to_idx = {letter: idx for idx, letter in enumerate(letters)}\n",
        "\n",
        "    training_data = {}\n",
        "\n",
        "    # Print available word lengths for debugging\n",
        "    available_lengths = sorted(words_by_length.keys())\n",
        "    print(f\"Available word lengths: {available_lengths}\")\n",
        "    print(f\"Lengths 22, 23, 24 present: {22 in words_by_length}, {23 in words_by_length}, {24 in words_by_length}\")\n",
        "\n",
        "    for length, word_list in words_by_length.items():\n",
        "        # Include all lengths including 22, 23, 24 if they have enough words\n",
        "        if len(word_list) < min_words_threshold:\n",
        "            print(f\"Skipping length {length}: only {len(word_list)} words (need at least {min_words_threshold})\")\n",
        "            continue\n",
        "\n",
        "        sequences = []\n",
        "        for word in word_list:\n",
        "            # Convert word to numerical sequence\n",
        "            seq = [letter_to_idx[char] for char in word]\n",
        "            sequences.append(seq)\n",
        "\n",
        "        training_data[length] = np.array(sequences)\n",
        "\n",
        "        print(f\"Length {length}: {len(sequences)} sequences\")\n",
        "\n",
        "    # Specifically check and report on lengths 22, 23, 24\n",
        "    for length in [22, 23, 24]:\n",
        "        if length in training_data:\n",
        "            print(f\"‚úì INCLUDED - Length {length}: {len(training_data[length])} sequences\")\n",
        "        elif length in words_by_length:\n",
        "            print(f\"‚úó EXCLUDED - Length {length}: {len(words_by_length[length])} words (below threshold)\")\n",
        "        else:\n",
        "            print(f\"‚úó NOT FOUND - Length {length}: No words in corpus\")\n",
        "\n",
        "    return training_data, letter_to_idx\n",
        "\n",
        "# Run with lower threshold to include more word lengths\n",
        "training_data, letter_to_idx = prepare_hmm_training_data(words_by_length, min_words_threshold=1)"
      ],
      "metadata": {
        "id": "w4jxdAW0bESj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b546f2-63cb-48ae-e0d5-77e5266d81ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available word lengths: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
            "Lengths 22, 23, 24 present: True, True, True\n",
            "Length 11: 5452 sequences\n",
            "Length 6: 3755 sequences\n",
            "Length 9: 6787 sequences\n",
            "Length 16: 698 sequences\n",
            "Length 14: 2019 sequences\n",
            "Length 10: 6465 sequences\n",
            "Length 8: 6348 sequences\n",
            "Length 12: 4292 sequences\n",
            "Length 13: 3094 sequences\n",
            "Length 5: 2340 sequences\n",
            "Length 18: 174 sequences\n",
            "Length 4: 1169 sequences\n",
            "Length 3: 388 sequences\n",
            "Length 7: 5111 sequences\n",
            "Length 15: 1226 sequences\n",
            "Length 17: 375 sequences\n",
            "Length 22: 8 sequences\n",
            "Length 19: 88 sequences\n",
            "Length 2: 84 sequences\n",
            "Length 1: 46 sequences\n",
            "Length 20: 40 sequences\n",
            "Length 21: 16 sequences\n",
            "Length 23: 3 sequences\n",
            "Length 24: 1 sequences\n",
            "‚úì INCLUDED - Length 22: 8 sequences\n",
            "‚úì INCLUDED - Length 23: 3 sequences\n",
            "‚úì INCLUDED - Length 24: 1 sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "muVwuh3GbKm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "100oXZOg8OVl",
        "outputId": "d12ff941-8fa4-4387-85c8-52ef612d0d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Initializing Hybrid CFP + Count-based HMM Model...\n",
            "üöÄ Training Count-based HMMs for Hybrid Model...\n",
            "   üìä Length 11: 5452 sequences\n",
            "   üìä Length 6: 3755 sequences\n",
            "   üìä Length 9: 6787 sequences\n",
            "   üìä Length 16: 698 sequences\n",
            "   üìä Length 14: 2019 sequences\n",
            "   üìä Length 10: 6465 sequences\n",
            "   üìä Length 8: 6348 sequences\n",
            "   üìä Length 12: 4292 sequences\n",
            "   üìä Length 13: 3094 sequences\n",
            "   üìä Length 5: 2340 sequences\n",
            "   üìä Length 18: 174 sequences\n",
            "   üìä Length 4: 1169 sequences\n",
            "   üìä Length 3: 388 sequences\n",
            "   üìä Length 7: 5111 sequences\n",
            "   üìä Length 15: 1226 sequences\n",
            "   üìä Length 17: 375 sequences\n",
            "   üìä Length 22: 8 sequences\n",
            "   üìä Length 19: 88 sequences\n",
            "   üìä Length 2: 84 sequences\n",
            "   üìä Length 1: 46 sequences\n",
            "   üìä Length 20: 40 sequences\n",
            "   üìä Length 21: 16 sequences\n",
            "   üìä Length 23: 3 sequences\n",
            "   üìä Length 24: 1 sequences\n",
            "‚úÖ Trained 24 count-based HMMs\n",
            "üéØ Hybrid Model Initialized: CFP weight=0.7, HMM weight=0.3\n",
            "\n",
            "============================================================\n",
            "üèÜ HYBRID MODEL READY!\n",
            "============================================================\n",
            "‚úÖ CFP Oracle: Word-based candidate filtering\n",
            "‚úÖ Count-based HMM: Statistical pattern learning\n",
            "‚úÖ Dynamic weighting: Adaptive confidence-based blending\n",
            "‚úÖ All word lengths supported\n",
            "‚úÖ Initial blend: 70% CFP + 30% HMM\n",
            "\n",
            "üéØ Ready for evaluation with 2000 games!\n"
          ]
        }
      ],
      "source": [
        "# Step 6: HYBRID CFP + Count-based HMM Training\n",
        "class HybridHangmanModel:\n",
        "    \"\"\"\n",
        "    Combines the best of CFP (accuracy) and HMM (generalization)\n",
        "    \"\"\"\n",
        "    def __init__(self, corpus_words, training_data, letter_to_idx, alpha=0.3):\n",
        "        self.cfp = CandidateFilterOracle(corpus_words)\n",
        "        self.count_hmms = train_count_hmm_models(training_data)\n",
        "        self.letter_to_idx = letter_to_idx\n",
        "        self.letters = string.ascii_uppercase\n",
        "        self.alpha = alpha  # Weight for HMM (0-1), CFP weight = 1-alpha\n",
        "\n",
        "        print(f\"üéØ Hybrid Model Initialized: CFP weight={1-alpha:.1f}, HMM weight={alpha:.1f}\")\n",
        "\n",
        "    def get_letter_probabilities(self, masked_word, guessed_letters):\n",
        "        \"\"\"\n",
        "        Combine CFP and Count-based HMM probabilities\n",
        "        \"\"\"\n",
        "        # Get CFP probabilities (high accuracy)\n",
        "        cfp_probs = self.cfp.get_letter_probabilities(masked_word, guessed_letters)\n",
        "\n",
        "        # Get HMM probabilities (good generalization)\n",
        "        hmm_probs = self._get_hmm_probabilities(masked_word, guessed_letters)\n",
        "\n",
        "        # If CFP has no candidates, rely more on HMM\n",
        "        if not cfp_probs or max(cfp_probs.values()) == 0:\n",
        "            return hmm_probs\n",
        "\n",
        "        # If HMM failed, rely on CFP\n",
        "        if not hmm_probs:\n",
        "            return cfp_probs\n",
        "\n",
        "        # Combine probabilities\n",
        "        hybrid_probs = {}\n",
        "        for letter in self.letters:\n",
        "            if letter not in guessed_letters:\n",
        "                cfp_prob = cfp_probs.get(letter, 0)\n",
        "                hmm_prob = hmm_probs.get(letter, 0)\n",
        "\n",
        "                # Dynamic weighting: if CFP is confident, trust it more\n",
        "                cfp_confidence = max(cfp_probs.values())\n",
        "                dynamic_alpha = self.alpha * (1 - cfp_confidence)  # Less HMM weight if CFP is confident\n",
        "\n",
        "                hybrid_prob = (1 - dynamic_alpha) * cfp_prob + dynamic_alpha * hmm_prob\n",
        "                hybrid_probs[letter] = hybrid_prob\n",
        "\n",
        "        # Normalize\n",
        "        total = sum(hybrid_probs.values())\n",
        "        if total > 0:\n",
        "            hybrid_probs = {letter: prob/total for letter, prob in hybrid_probs.items()}\n",
        "\n",
        "        return hybrid_probs\n",
        "\n",
        "    def _get_hmm_probabilities(self, masked_word, guessed_letters):\n",
        "        \"\"\"Get probabilities from count-based HMM\"\"\"\n",
        "        word_length = len(masked_word)\n",
        "\n",
        "        if word_length not in self.count_hmms:\n",
        "            return {}\n",
        "\n",
        "        model = self.count_hmms[word_length]\n",
        "\n",
        "        # Prepare observations\n",
        "        observations = []\n",
        "        for char in masked_word:\n",
        "            if char == '_':\n",
        "                observations.append(-1)  # Missing\n",
        "            else:\n",
        "                observations.append(self.letter_to_idx[char])\n",
        "\n",
        "        try:\n",
        "            # Get posteriors using count-based HMM\n",
        "            posteriors = model.forward_backward(observations)\n",
        "\n",
        "            # Aggregate probabilities for blank positions\n",
        "            letter_probs = np.zeros(26)\n",
        "            for pos, char in enumerate(masked_word):\n",
        "                if char == '_':\n",
        "                    for letter_idx in range(26):\n",
        "                        letter = self.idx_to_letter(letter_idx)\n",
        "                        if letter not in guessed_letters:\n",
        "                            letter_probs[letter_idx] += posteriors[pos, letter_idx]\n",
        "\n",
        "            # Normalize\n",
        "            if np.sum(letter_probs) > 0:\n",
        "                letter_probs /= np.sum(letter_probs)\n",
        "\n",
        "            return {self.idx_to_letter(i): letter_probs[i]\n",
        "                    for i in range(26) if self.idx_to_letter(i) not in guessed_letters}\n",
        "\n",
        "        except Exception as e:\n",
        "            return {}\n",
        "\n",
        "    def idx_to_letter(self, idx):\n",
        "        \"\"\"Convert index to letter\"\"\"\n",
        "        return chr(65 + idx)\n",
        "\n",
        "# COUNT-BASED HMM IMPLEMENTATION (from previous)\n",
        "class CountBasedHMM:\n",
        "    def __init__(self):\n",
        "        self.transition_probs = None\n",
        "        self.emission_probs = None\n",
        "        self.initial_probs = None\n",
        "\n",
        "    def train(self, sequences, alpha=0.1):\n",
        "        n_states = 26\n",
        "\n",
        "        # Initialize counts with smoothing\n",
        "        transition_counts = np.ones((n_states, n_states)) * alpha\n",
        "        emission_counts = np.ones((n_states, n_states)) * alpha\n",
        "        initial_counts = np.ones(n_states) * alpha\n",
        "\n",
        "        # Count transitions and emissions\n",
        "        for seq in sequences:\n",
        "            if len(seq) > 0:\n",
        "                initial_counts[seq[0]] += 1\n",
        "\n",
        "            for i in range(len(seq)):\n",
        "                emission_counts[seq[i], seq[i]] += 1\n",
        "                if i < len(seq) - 1:\n",
        "                    transition_counts[seq[i], seq[i+1]] += 1\n",
        "\n",
        "        # Convert to probabilities\n",
        "        self.initial_probs = initial_counts / np.sum(initial_counts)\n",
        "        self.transition_probs = transition_counts / np.sum(transition_counts, axis=1, keepdims=True)\n",
        "        self.emission_probs = emission_counts / np.sum(emission_counts, axis=1, keepdims=True)\n",
        "\n",
        "    def forward_backward(self, observations):\n",
        "        n_positions = len(observations)\n",
        "        n_states = 26\n",
        "\n",
        "        # Forward pass\n",
        "        forward = np.zeros((n_positions, n_states))\n",
        "        for state in range(n_states):\n",
        "            if observations[0] == -1:\n",
        "                forward[0, state] = self.initial_probs[state]\n",
        "            else:\n",
        "                forward[0, state] = self.initial_probs[state] * self.emission_probs[state, observations[0]]\n",
        "\n",
        "        for t in range(1, n_positions):\n",
        "            for j in range(n_states):\n",
        "                sum_prob = 0\n",
        "                for i in range(n_states):\n",
        "                    sum_prob += forward[t-1, i] * self.transition_probs[i, j]\n",
        "\n",
        "                if observations[t] == -1:\n",
        "                    forward[t, j] = sum_prob\n",
        "                else:\n",
        "                    forward[t, j] = sum_prob * self.emission_probs[j, observations[t]]\n",
        "\n",
        "        # Backward pass\n",
        "        backward = np.ones((n_positions, n_states))\n",
        "        for t in range(n_positions-2, -1, -1):\n",
        "            for i in range(n_states):\n",
        "                sum_prob = 0\n",
        "                for j in range(n_states):\n",
        "                    if observations[t+1] == -1:\n",
        "                        emission_prob = 1.0\n",
        "                    else:\n",
        "                        emission_prob = self.emission_probs[j, observations[t+1]]\n",
        "                    sum_prob += self.transition_probs[i, j] * emission_prob * backward[t+1, j]\n",
        "                backward[t, i] = sum_prob\n",
        "\n",
        "        # Combine\n",
        "        posteriors = forward * backward\n",
        "        posteriors = posteriors / np.sum(posteriors, axis=1, keepdims=True)\n",
        "        return posteriors\n",
        "\n",
        "\n",
        "def train_count_hmm_models(training_data):\n",
        "    \"\"\"Train count-based HMMs for all word lengths\"\"\"\n",
        "    count_hmms = {}\n",
        "\n",
        "    print(\"üöÄ Training Count-based HMMs for Hybrid Model...\")\n",
        "    for length, sequences in training_data.items():\n",
        "        if len(sequences) < 1:\n",
        "            continue\n",
        "\n",
        "        print(f\"   üìä Length {length}: {len(sequences)} sequences\")\n",
        "        model = CountBasedHMM()\n",
        "        model.train(sequences, alpha=0.1)\n",
        "        count_hmms[length] = model\n",
        "\n",
        "    print(f\"‚úÖ Trained {len(count_hmms)} count-based HMMs\")\n",
        "    return count_hmms\n",
        "\n",
        "# CFP ORACLE (from previous)\n",
        "class CandidateFilterOracle:\n",
        "    def __init__(self, corpus_words):\n",
        "        self.by_len = defaultdict(list)\n",
        "        for word in corpus_words:\n",
        "            w = word.strip().upper()\n",
        "            if w and w.isalpha():\n",
        "                self.by_len[len(w)].append(w)\n",
        "\n",
        "    @staticmethod\n",
        "    def matches_mask(word, mask, wrong_letters):\n",
        "        if len(word) != len(mask):\n",
        "            return False\n",
        "        if any(letter in word for letter in wrong_letters):\n",
        "            return False\n",
        "        for wc, mc in zip(word, mask):\n",
        "            if mc != '_' and wc != mc:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def get_letter_probabilities(self, masked_word, guessed_letters):\n",
        "        word_length = len(masked_word)\n",
        "        revealed_letters = set(char for char in masked_word if char != '_')\n",
        "        wrong_letters = guessed_letters - revealed_letters\n",
        "\n",
        "        # Get candidate words\n",
        "        candidates = [\n",
        "            word for word in self.by_len.get(word_length, [])\n",
        "            if self.matches_mask(word, masked_word, wrong_letters)\n",
        "        ]\n",
        "\n",
        "        if not candidates:\n",
        "            return self._fallback_probs(guessed_letters)\n",
        "\n",
        "        # Count letter frequencies in blank positions\n",
        "        blank_positions = [i for i, char in enumerate(masked_word) if char == '_']\n",
        "        letter_counts = Counter()\n",
        "\n",
        "        for word in candidates:\n",
        "            for pos in blank_positions:\n",
        "                letter_counts[word[pos]] += 1\n",
        "\n",
        "        # Convert to probabilities\n",
        "        total_count = sum(letter_counts.values())\n",
        "        prob_dict = {}\n",
        "        for letter in string.ascii_uppercase:\n",
        "            if letter not in guessed_letters:\n",
        "                prob_dict[letter] = letter_counts[letter] / total_count if total_count > 0 else 0\n",
        "\n",
        "        # Normalize\n",
        "        total_prob = sum(prob_dict.values())\n",
        "        if total_prob > 0:\n",
        "            prob_dict = {letter: prob/total_prob for letter, prob in prob_dict.items()}\n",
        "        else:\n",
        "            prob_dict = self._fallback_probs(guessed_letters)\n",
        "\n",
        "        return prob_dict\n",
        "\n",
        "    def _fallback_probs(self, guessed_letters):\n",
        "        available = [l for l in string.ascii_uppercase if l not in guessed_letters]\n",
        "        prob = 1.0 / len(available) if available else 0\n",
        "        return {letter: prob for letter in available}\n",
        "\n",
        "# üöÄ INITIALIZE HYBRID MODEL\n",
        "print(\"üéØ Initializing Hybrid CFP + Count-based HMM Model...\")\n",
        "hybrid_model = HybridHangmanModel(words, training_data, letter_to_idx, alpha=0.3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÜ HYBRID MODEL READY!\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ CFP Oracle: Word-based candidate filtering\")\n",
        "print(\"‚úÖ Count-based HMM: Statistical pattern learning\")\n",
        "print(\"‚úÖ Dynamic weighting: Adaptive confidence-based blending\")\n",
        "print(\"‚úÖ All word lengths supported\")\n",
        "print(f\"‚úÖ Initial blend: {70}% CFP + {30}% HMM\")\n",
        "\n",
        "# Update your main HMM reference to use the hybrid model\n",
        "hangman_hmm = hybrid_model\n",
        "\n",
        "print(\"\\nüéØ Ready for evaluation with 2000 games!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Test the HMM with example game states\n",
        "def test_hmm_predictions():\n",
        "    \"\"\"Test HMM predictions with various game states\"\"\"\n",
        "\n",
        "    test_cases = [\n",
        "        # (masked_word, guessed_letters, description)\n",
        "        (['_', '_', '_', '_', '_'], set(), \"Empty 5-letter word\"),\n",
        "        (['A', '_', '_', '_', '_'], set('A'), \"5-letter word starting with A\"),\n",
        "        (['_', '_', '_', 'E', '_'], set('E'), \"5-letter word with E at position 3\"),\n",
        "        (['_', '_', '_', '_', '_', '_'], set(), \"Empty 6-letter word\"),\n",
        "    ]\n",
        "\n",
        "    for masked_word, guessed_letters, description in test_cases:\n",
        "        print(f\"\\n{description}:\")\n",
        "        print(f\"Masked word: {' '.join(masked_word)}\")\n",
        "        print(f\"Guessed letters: {''.join(sorted(guessed_letters))}\")\n",
        "\n",
        "        probs = hangman_hmm.get_letter_probabilities(masked_word, guessed_letters)\n",
        "\n",
        "        # Show top 5 predictions\n",
        "        top_letters = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        print(\"Top 5 letter predictions:\")\n",
        "        for letter, prob in top_letters:\n",
        "            print(f\"  {letter}: {prob:.4f}\")\n",
        "\n",
        "test_hmm_predictions()"
      ],
      "metadata": {
        "id": "z1APELXJbXA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f23005-8ac1-48d6-9871-13877e305b45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Empty 5-letter word:\n",
            "Masked word: _ _ _ _ _\n",
            "Guessed letters: \n",
            "Top 5 letter predictions:\n",
            "  A: 0.1073\n",
            "  E: 0.0981\n",
            "  O: 0.0694\n",
            "  R: 0.0676\n",
            "  I: 0.0634\n",
            "\n",
            "5-letter word starting with A:\n",
            "Masked word: A _ _ _ _\n",
            "Guessed letters: A\n",
            "Top 5 letter predictions:\n",
            "  E: 0.1023\n",
            "  I: 0.0946\n",
            "  O: 0.0846\n",
            "  N: 0.0731\n",
            "  S: 0.0701\n",
            "\n",
            "5-letter word with E at position 3:\n",
            "Masked word: _ _ _ E _\n",
            "Guessed letters: E\n",
            "Top 5 letter predictions:\n",
            "  R: 0.1329\n",
            "  A: 0.0755\n",
            "  N: 0.0711\n",
            "  L: 0.0675\n",
            "  T: 0.0672\n",
            "\n",
            "Empty 6-letter word:\n",
            "Masked word: _ _ _ _ _ _\n",
            "Guessed letters: \n",
            "Top 5 letter predictions:\n",
            "  E: 0.1116\n",
            "  A: 0.1021\n",
            "  R: 0.0712\n",
            "  I: 0.0709\n",
            "  N: 0.0615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: OFFICIAL EVALUATION - 2000 Games with Official Scoring Formula\n",
        "def official_evaluation(hangman_hmm, test_words, num_games=2000, max_wrong_guesses=6):\n",
        "    \"\"\"\n",
        "    Official evaluation as per problem statement\n",
        "    Plays 2000 games and calculates the official score\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from collections import defaultdict\n",
        "\n",
        "    print(\"üéØ OFFICIAL EVALUATION - 2000 HANGMAN GAMES\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Playing {num_games} games with {max_wrong_guesses} wrong guesses allowed per game\")\n",
        "    print()\n",
        "\n",
        "    # Initialize metrics\n",
        "    results = {\n",
        "        'games_won': 0,\n",
        "        'games_lost': 0,\n",
        "        'total_wrong_guesses': 0,\n",
        "        'total_repeated_guesses': 0,\n",
        "        'games_details': [],\n",
        "        'score_breakdown': defaultdict(int)\n",
        "    }\n",
        "\n",
        "    # Play specified number of games\n",
        "    for game_id in range(min(num_games, len(test_words))):\n",
        "        if game_id % 500 == 0:\n",
        "            print(f\"üéÆ Played {game_id}/{num_games} games...\")\n",
        "\n",
        "        target_word = test_words[game_id]\n",
        "        game_result = play_single_game(hangman_hmm, target_word, max_wrong_guesses, game_id + 1)\n",
        "\n",
        "        # Aggregate results\n",
        "        if game_result['won']:\n",
        "            results['games_won'] += 1\n",
        "        else:\n",
        "            results['games_lost'] += 1\n",
        "\n",
        "        results['total_wrong_guesses'] += game_result['wrong_guesses']\n",
        "        results['total_repeated_guesses'] += game_result['repeated_guesses']\n",
        "        results['games_details'].append(game_result)\n",
        "\n",
        "    # Calculate official score\n",
        "    official_score = calculate_official_score(results, num_games)\n",
        "\n",
        "    # Print comprehensive results\n",
        "    print_official_results(results, official_score, num_games)\n",
        "\n",
        "    return results, official_score\n",
        "\n",
        "def play_single_game(hangman_hmm, target_word, max_wrong_guesses, game_id):\n",
        "    \"\"\"\n",
        "    Play a single Hangman game and return detailed results\n",
        "    \"\"\"\n",
        "    masked_word = ['_'] * len(target_word)\n",
        "    guessed_letters = set()\n",
        "    wrong_guesses = 0\n",
        "    repeated_guesses = 0\n",
        "    game_log = []\n",
        "\n",
        "    # Game loop\n",
        "    while wrong_guesses < max_wrong_guesses and '_' in masked_word:\n",
        "        # Get letter probabilities from HMM\n",
        "        probs = hangman_hmm.get_letter_probabilities(masked_word, guessed_letters)\n",
        "\n",
        "        if not probs:\n",
        "            # No probabilities available, use fallback\n",
        "            available_letters = [chr(i) for i in range(65, 91) if chr(i) not in guessed_letters]\n",
        "            if not available_letters:\n",
        "                break\n",
        "            next_letter = available_letters[0]\n",
        "        else:\n",
        "            # Choose letter with highest probability\n",
        "            next_letter = max(probs.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "        # Check for repeated guess\n",
        "        if next_letter in guessed_letters:\n",
        "            repeated_guesses += 1\n",
        "            game_log.append(f\"Repeated guess: {next_letter}\")\n",
        "            continue\n",
        "\n",
        "        # Add to guessed letters\n",
        "        guessed_letters.add(next_letter)\n",
        "\n",
        "        # Check if letter is in target word\n",
        "        if next_letter in target_word:\n",
        "            # Update masked word\n",
        "            for i, char in enumerate(target_word):\n",
        "                if char == next_letter:\n",
        "                    masked_word[i] = next_letter\n",
        "            game_log.append(f\"Correct: {next_letter} -> {' '.join(masked_word)}\")\n",
        "        else:\n",
        "            wrong_guesses += 1\n",
        "            game_log.append(f\"Wrong: {next_letter} ({wrong_guesses}/{max_wrong_guesses} wrong)\")\n",
        "\n",
        "    # Determine game outcome\n",
        "    won = '_' not in masked_word\n",
        "    actual_word = ''.join(target_word)\n",
        "    guessed_word = ''.join(masked_word)\n",
        "\n",
        "    return {\n",
        "        'game_id': game_id,\n",
        "        'target_word': actual_word,\n",
        "        'won': won,\n",
        "        'wrong_guesses': wrong_guesses,\n",
        "        'repeated_guesses': repeated_guesses,\n",
        "        'total_guesses': len(guessed_letters),\n",
        "        'final_word': guessed_word,\n",
        "        'game_log': game_log\n",
        "    }\n",
        "\n",
        "def calculate_official_score(results, num_games):\n",
        "    \"\"\"\n",
        "    Calculate official score using the formula:\n",
        "    Final Score = (Success Rate * 2000) - (Total Wrong Guesses * 5) - (Total Repeated Guesses * 2)\n",
        "    \"\"\"\n",
        "    success_rate = results['games_won'] / num_games\n",
        "    total_wrong_guesses = results['total_wrong_guesses']\n",
        "    total_repeated_guesses = results['total_repeated_guesses']\n",
        "\n",
        "    score = (success_rate * 2000) - (total_wrong_guesses * 5) - (total_repeated_guesses * 2)\n",
        "\n",
        "    # Store breakdown for analysis\n",
        "    results['score_breakdown']['success_component'] = success_rate * 2000\n",
        "    results['score_breakdown']['wrong_penalty'] = total_wrong_guesses * 5\n",
        "    results['score_breakdown']['repeated_penalty'] = total_repeated_guesses * 2\n",
        "    results['score_breakdown']['success_rate'] = success_rate\n",
        "    results['score_breakdown']['total_games'] = num_games\n",
        "\n",
        "    return score\n",
        "\n",
        "def print_official_results(results, official_score, num_games):\n",
        "    \"\"\"\n",
        "    Print comprehensive evaluation results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üèÜ OFFICIAL EVALUATION RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"üìä GAME STATISTICS:\")\n",
        "    print(f\"   Total Games Played: {num_games}\")\n",
        "    print(f\"   Games Won: {results['games_won']}\")\n",
        "    print(f\"   Games Lost: {results['games_lost']}\")\n",
        "    print(f\"   Success Rate: {results['games_won']/num_games:.3f} ({results['games_won']}/{num_games})\")\n",
        "\n",
        "    print(f\"\\nüéØ PERFORMANCE METRICS:\")\n",
        "    print(f\"   Total Wrong Guesses: {results['total_wrong_guesses']}\")\n",
        "    print(f\"   Total Repeated Guesses: {results['total_repeated_guesses']}\")\n",
        "    print(f\"   Average Wrong Guesses per Game: {results['total_wrong_guesses']/num_games:.2f}\")\n",
        "    print(f\"   Average Repeated Guesses per Game: {results['total_repeated_guesses']/num_games:.2f}\")\n",
        "\n",
        "    print(f\"\\nüí∞ SCORE BREAKDOWN:\")\n",
        "    print(f\"   Success Component: {results['score_breakdown']['success_component']:.2f}\")\n",
        "    print(f\"   Wrong Guesses Penalty: -{results['score_breakdown']['wrong_penalty']:.2f}\")\n",
        "    print(f\"   Repeated Guesses Penalty: -{results['score_breakdown']['repeated_penalty']:.2f}\")\n",
        "    print(f\"   FINAL SCORE: {official_score:.2f}\")\n",
        "\n",
        "    # Additional analysis\n",
        "    print(f\"\\nüìà ADDITIONAL INSIGHTS:\")\n",
        "\n",
        "    # Word length analysis\n",
        "    won_by_length = defaultdict(int)\n",
        "    lost_by_length = defaultdict(int)\n",
        "\n",
        "    for game in results['games_details']:\n",
        "        length = len(game['target_word'])\n",
        "        if game['won']:\n",
        "            won_by_length[length] += 1\n",
        "        else:\n",
        "            lost_by_length[length] += 1\n",
        "\n",
        "    print(f\"   Performance by Word Length:\")\n",
        "    for length in sorted(set(won_by_length.keys()) | set(lost_by_length.keys())):\n",
        "        won = won_by_length[length]\n",
        "        total = won + lost_by_length.get(length, 0)\n",
        "        if total > 0:\n",
        "            success_rate = won / total\n",
        "            print(f\"     Length {length}: {won}/{total} won ({success_rate:.1%})\")\n",
        "\n",
        "    # Efficiency analysis\n",
        "    total_correct_guesses = sum(game['total_guesses'] - game['wrong_guesses'] for game in results['games_details'])\n",
        "    total_guesses = sum(game['total_guesses'] for game in results['games_details'])\n",
        "    efficiency = total_correct_guesses / total_guesses if total_guesses > 0 else 0\n",
        "\n",
        "    print(f\"   Guess Efficiency: {efficiency:.1%} ({total_correct_guesses}/{total_guesses} correct guesses)\")\n",
        "\n",
        "# Load test words for evaluation\n",
        "def load_test_words_for_evaluation(file_path, num_words=2000):\n",
        "    \"\"\"Load test words for official evaluation\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        words = [line.strip().upper() for line in f if line.strip()]\n",
        "\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "\n",
        "    if len(words) < num_words:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Only {len(words)} test words available, using all of them\")\n",
        "        return words\n",
        "    else:\n",
        "        return words[:num_words]\n",
        "\n",
        "# üöÄ RUN OFFICIAL EVALUATION\n",
        "print(\"Loading test words for official evaluation...\")\n",
        "test_words_eval = load_test_words_for_evaluation('test.txt', num_words=2000)\n",
        "\n",
        "print(f\"Loaded {len(test_words_eval)} test words for evaluation\")\n",
        "print(\"Starting official 2000-game evaluation...\")\n",
        "\n",
        "# Run the official evaluation\n",
        "results, official_score = official_evaluation(hangman_hmm, test_words_eval, num_games=2000, max_wrong_guesses=6)\n",
        "\n",
        "# Save detailed results\n",
        "def save_detailed_results(results, official_score, filename=\"evaluation_results.txt\"):\n",
        "    \"\"\"Save detailed evaluation results to file\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"OFFICIAL HANGMAN EVALUATION RESULTS\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"FINAL SCORE: {official_score:.2f}\\n\\n\")\n",
        "\n",
        "        f.write(\"SUMMARY STATISTICS:\\n\")\n",
        "        f.write(f\"Games Played: {results['score_breakdown']['total_games']}\\n\")\n",
        "        f.write(f\"Games Won: {results['games_won']}\\n\")\n",
        "        f.write(f\"Success Rate: {results['score_breakdown']['success_rate']:.3f}\\n\")\n",
        "        f.write(f\"Total Wrong Guesses: {results['total_wrong_guesses']}\\n\")\n",
        "        f.write(f\"Total Repeated Guesses: {results['total_repeated_guesses']}\\n\\n\")\n",
        "\n",
        "        f.write(\"SCORE BREAKDOWN:\\n\")\n",
        "        f.write(f\"Success Component: {results['score_breakdown']['success_component']:.2f}\\n\")\n",
        "        f.write(f\"Wrong Guesses Penalty: -{results['score_breakdown']['wrong_penalty']:.2f}\\n\")\n",
        "        f.write(f\"Repeated Guesses Penalty: -{results['score_breakdown']['repeated_penalty']:.2f}\\n\\n\")\n",
        "\n",
        "        f.write(\"FIRST 10 GAME RESULTS:\\n\")\n",
        "        for game in results['games_details'][:10]:\n",
        "            status = \"WON\" if game['won'] else \"LOST\"\n",
        "            f.write(f\"Game {game['game_id']}: {game['target_word']} -> {status} \"\n",
        "                   f\"(Wrong: {game['wrong_guesses']}, Repeated: {game['repeated_guesses']})\\n\")\n",
        "\n",
        "    print(f\"üìÑ Detailed results saved to {filename}\")\n",
        "\n",
        "# Save results\n",
        "save_detailed_results(results, official_score)\n",
        "\n",
        "# Performance assessment\n",
        "def assess_performance(score):\n",
        "    \"\"\"Assess the performance based on final score\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üìã PERFORMANCE ASSESSMENT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if score >= 1500:\n",
        "        assessment = \"EXCELLENT üèÜ\"\n",
        "        feedback = \"Outstanding performance! Your agent is highly efficient.\"\n",
        "    elif score >= 1000:\n",
        "        assessment = \"VERY GOOD ü•à\"\n",
        "        feedback = \"Strong performance with good success rate and efficiency.\"\n",
        "    elif score >= 500:\n",
        "        assessment = \"GOOD ü•â\"\n",
        "        feedback = \"Solid performance with room for optimization.\"\n",
        "    elif score >= 0:\n",
        "        assessment = \"FAIR üìä\"\n",
        "        feedback = \"Basic functionality achieved, needs improvement in efficiency.\"\n",
        "    else:\n",
        "        assessment = \"NEEDS WORK üîß\"\n",
        "        feedback = \"Focus on improving success rate and reducing wrong guesses.\"\n",
        "\n",
        "    print(f\"FINAL SCORE: {score:.2f}\")\n",
        "    print(f\"ASSESSMENT: {assessment}\")\n",
        "    print(f\"FEEDBACK: {feedback}\")\n",
        "\n",
        "# Final assessment\n",
        "assess_performance(official_score)\n",
        "\n",
        "print(f\"\\nüéâ EVALUATION COMPLETE!\")\n",
        "print(f\"üìä Check 'evaluation_results.txt' for detailed game-by-game results\")"
      ],
      "metadata": {
        "id": "4wanux8QS0PJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3be4a79-cc8e-4a6f-b23f-4f7840ac3a19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading test words for official evaluation...\n",
            "Loaded 2000 test words for evaluation\n",
            "Starting official 2000-game evaluation...\n",
            "üéØ OFFICIAL EVALUATION - 2000 HANGMAN GAMES\n",
            "============================================================\n",
            "Playing 2000 games with 6 wrong guesses allowed per game\n",
            "\n",
            "üéÆ Played 0/2000 games...\n",
            "üéÆ Played 500/2000 games...\n",
            "üéÆ Played 1000/2000 games...\n",
            "üéÆ Played 1500/2000 games...\n",
            "\n",
            "======================================================================\n",
            "üèÜ OFFICIAL EVALUATION RESULTS\n",
            "======================================================================\n",
            "üìä GAME STATISTICS:\n",
            "   Total Games Played: 2000\n",
            "   Games Won: 703\n",
            "   Games Lost: 1297\n",
            "   Success Rate: 0.351 (703/2000)\n",
            "\n",
            "üéØ PERFORMANCE METRICS:\n",
            "   Total Wrong Guesses: 10213\n",
            "   Total Repeated Guesses: 0\n",
            "   Average Wrong Guesses per Game: 5.11\n",
            "   Average Repeated Guesses per Game: 0.00\n",
            "\n",
            "üí∞ SCORE BREAKDOWN:\n",
            "   Success Component: 703.00\n",
            "   Wrong Guesses Penalty: -51065.00\n",
            "   Repeated Guesses Penalty: -0.00\n",
            "   FINAL SCORE: -50362.00\n",
            "\n",
            "üìà ADDITIONAL INSIGHTS:\n",
            "   Performance by Word Length:\n",
            "     Length 2: 0/2 won (0.0%)\n",
            "     Length 3: 0/9 won (0.0%)\n",
            "     Length 4: 4/37 won (10.8%)\n",
            "     Length 5: 10/91 won (11.0%)\n",
            "     Length 6: 24/138 won (17.4%)\n",
            "     Length 7: 39/205 won (19.0%)\n",
            "     Length 8: 63/246 won (25.6%)\n",
            "     Length 9: 75/274 won (27.4%)\n",
            "     Length 10: 108/282 won (38.3%)\n",
            "     Length 11: 102/226 won (45.1%)\n",
            "     Length 12: 84/164 won (51.2%)\n",
            "     Length 13: 67/128 won (52.3%)\n",
            "     Length 14: 43/86 won (50.0%)\n",
            "     Length 15: 35/47 won (74.5%)\n",
            "     Length 16: 25/33 won (75.8%)\n",
            "     Length 17: 13/17 won (76.5%)\n",
            "     Length 18: 6/8 won (75.0%)\n",
            "     Length 19: 3/3 won (100.0%)\n",
            "     Length 20: 1/2 won (50.0%)\n",
            "     Length 21: 0/1 won (0.0%)\n",
            "     Length 22: 1/1 won (100.0%)\n",
            "   Guess Efficiency: 55.2% (12572/22785 correct guesses)\n",
            "üìÑ Detailed results saved to evaluation_results.txt\n",
            "\n",
            "==================================================\n",
            "üìã PERFORMANCE ASSESSMENT\n",
            "==================================================\n",
            "FINAL SCORE: -50362.00\n",
            "ASSESSMENT: NEEDS WORK üîß\n",
            "FEEDBACK: Focus on improving success rate and reducing wrong guesses.\n",
            "\n",
            "üéâ EVALUATION COMPLETE!\n",
            "üìä Check 'evaluation_results.txt' for detailed game-by-game results\n"
          ]
        }
      ]
    }
  ]
}